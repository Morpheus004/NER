{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the dataset ner_dataset.csv from https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./archive/ner_dataset.csv', encoding= 'unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sentences are broken into tokens in the column 'Word'. The column 'sentence #' displays the sentence number once and then prints NaN till the next sentence begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract mappings required for the neural network\n",
    "To train a neural network, we will use two mappings as given below. The neural network will only take integers as input. So lets convert all the unique tokens in the corpus to its respective index.\n",
    "- {token} to {token id}: address the row in embeddings matrix for the current token.\n",
    "- {tag} to {tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>22712</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>32833</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>6038</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>23582</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>19558</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  Word_idx  Tag_idx\n",
       "0  Sentence: 1      Thousands  NNS   O     22712       16\n",
       "1          NaN             of   IN   O     32833       16\n",
       "2          NaN  demonstrators  NNS   O      6038       16\n",
       "3          NaN           have  VBP   O     23582       16\n",
       "4          NaN        marched  VBN   O     19558       16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transform columns to extract sequential data\n",
    "Next, lets fill NaN in 'sentence #' column using method ffill in fillna. Thereafter groupby on the sentence column to get a list of tokens and tags for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #     True\n",
       "Word           True\n",
       "POS           False\n",
       "Tag           False\n",
       "Word_idx      False\n",
       "Tag_idx       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_19584\\450957418.py:1: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_fillna = data.fillna(method='ffill', axis=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "      <td>[22712, 32833, 6038, 23582, 19558, 13631, 2128...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 2, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[31258, 775, 22676, 15913, 31158, 4550, 33501,...</td>\n",
       "      <td>[5, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "      <td>[30771, 11600, 32572, 24898, 3426, 8923, 15313...</td>\n",
       "      <td>[16, 16, 8, 16, 16, 16, 16, 16, 2, 16, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[34797, 17432, 16834, 7186, 1875, 12516, 28168...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "      <td>[894, 22131, 34707, 32557, 19922, 11238, 16724...</td>\n",
       "      <td>[2, 16, 16, 0, 15, 16, 8, 16, 2, 16, 5, 16, 5,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #                                               Word  \\\n",
       "0      Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1     Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
       "2    Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "3   Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
       "4  Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
       "1  [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
       "2  [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
       "3     [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
       "4  [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
       "\n",
       "                                                 Tag  \\\n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...   \n",
       "1  [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "2  [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...   \n",
       "3                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...   \n",
       "\n",
       "                                            Word_idx  \\\n",
       "0  [22712, 32833, 6038, 23582, 19558, 13631, 2128...   \n",
       "1  [31258, 775, 22676, 15913, 31158, 4550, 33501,...   \n",
       "2  [30771, 11600, 32572, 24898, 3426, 8923, 15313...   \n",
       "3  [34797, 17432, 16834, 7186, 1875, 12516, 28168...   \n",
       "4  [894, 22131, 34707, 32557, 19922, 11238, 16724...   \n",
       "\n",
       "                                             Tag_idx  \n",
       "0  [16, 16, 16, 16, 16, 16, 2, 16, 16, 16, 16, 16...  \n",
       "1  [5, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
       "2  [16, 16, 8, 16, 16, 16, 16, 16, 2, 16, 16, 16,...  \n",
       "3       [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]  \n",
       "4  [2, 16, 16, 0, 15, 16, 8, 16, 2, 16, 5, 16, 5,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "data_group = data_fillna.groupby(['Sentence #'], as_index=False)[['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx']].agg(lambda x: list(x))\n",
    "\n",
    "data_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pad sequences and split the dataset into train, test\n",
    "Padding: The LSTM layers accept sequences of same length only. Therefore we will want to transform our list of token_sequences ('Word_idx') which is lists of integers into a matrix of shape (token_sequences, max_len). We can use any length as max_len. In this project we will be using length of the longest sequence as max_len. The sequences that are shorter than max_len are padded with a specified value at the end.\n",
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 43163 \n",
      "test_tokens length: 4796 \n",
      "train_tags: 43163 \n",
      "test_tags: 4796\n"
     ]
    }
   ],
   "source": [
    "def get_pad_train_test_val(data_group, data):\n",
    "    n_token = len(list(set(data['Word'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "    \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "    \n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    \n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    \n",
    "    train_tokens, test_tokens, train_tags, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntrain_tags:', len(train_tags),\n",
    "        '\\ntest_tags:', len(test_tags)\n",
    "    )\n",
    "    \n",
    "    return train_tokens, test_tokens, train_tags, test_tags\n",
    "\n",
    "train_tokens, test_tokens, train_tags, test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4279\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "22676\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "716\t[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "18713\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "10742\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "24401\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "4550\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "21969\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "16088\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "6269\t[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "10050\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "6513\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "15913\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "12133\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "33331\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "32728\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "32056\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "9495\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "22688\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "22977\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "15313\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "27655\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "25203\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "4330\t[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "27014\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "15921\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "22893\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "35177\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "for token, tag in zip(train_tokens[0], train_tags[0]):\n",
    "    print('%s\\t%s' % (token, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Model Layout\n",
    "\n",
    "Lets go through the process of building a neural network model with lstm layers. Please compare the layers brief and model plot given below to get a better understanding of the layers, input and output dimensions. We are building a simple model with 4 layers.\n",
    "\n",
    "- **Layer 1 - Embedding layer** : We will feed the padded sequences of equal length (104) to the embedding layer. Once the network has been trained, each token will get transformed into a vector of n dimensions. We have chosen the n dimensions to be (64). \n",
    "\n",
    "\n",
    "These are the dimensions (?, 104, 64) plotted in the model plot for input layer and embedding layer. The ? or None in the dimension specifies batches, when it is None or ? the model can take any batch size.\n",
    "\n",
    "- **Layer 2 - Bidirectional LSTM** : Bidirectional lstm takes a recurrent layer (e.g. the first LSTM layer) as an argument. This layer takes the output from the previous embedding layer (104, 64). It also allows you to specify the merge mode, that is how the forward and backward outputs should be combined before being passed on to the next layer. The default mode is to concatenate, where the outputs are concatenated together, providing double the number of outputs to the next layer, in our case its 128(64 * 2).\n",
    "\n",
    "\n",
    "The output dimension of the bidirectional lstm layer (?, 104, 128) becomes the input dimension of the next lstm layer.\n",
    "\n",
    "- **Layer 3 - LSTM Layer** : An LSTM network is a recurrent neural network that has LSTM cell blocks in place of our standard neural network layers. These cells have various components called the input gate, the forget gate and the output gate.\n",
    "\n",
    "\n",
    "This layer takes the output dimension from the previous bidirectional lstm layer (?, 104, 128) and outputs (?, 104, 256)\n",
    "\n",
    "\n",
    "\n",
    "- **Layer 4 - TimeDistributed  Layer** : We are dealing with Many to Many RNN Architecture where we expect output from every input sequence for example (a1 →b1, a2 →b2… an →bn) where a and b are inputs and outputs of every sequence. The TimeDistributeDense layers allow you to apply Dense(fully-connected) operation across every output over every time-step. If you don't use this, you would only have one final output.\n",
    "\n",
    "\n",
    "\n",
    "This layer take the output dimension of the previous lstm layer (104, 256) and outputs the max sequence length (104) and max tags (17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# import keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  35179 \n",
      "output_dim:  32 \n",
      "input_length:  104 \n",
      "n_tags:  17\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(list(set(data['Word'].to_list())))+1\n",
    "output_dim = 32\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "    #Optimiser \n",
    "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ time_distributed_1 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m      8\u001b[0m model_bilstm_lstm \u001b[38;5;241m=\u001b[39m get_bilstm_lstm_model()\n\u001b[1;32m----> 9\u001b[0m plot_model(model_bilstm_lstm)\n",
      "File \u001b[1;32mE:\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\model_visualization.py:421\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, show_layer_activations, show_trainable, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a Keras model to dot format and save to a file.\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    This enables in-line display of the model plots in notebooks.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m     )\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_pydot():\n\u001b[0;32m    427\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor `plot_model` to work.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for _ in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "plot_model(model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - accuracy: 0.6975 - loss: 2.1064 - val_accuracy: 0.9679 - val_loss: 0.3765\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.9677 - loss: 0.4074 - val_accuracy: 0.9679 - val_loss: 0.3524\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9677 - loss: 0.3812 - val_accuracy: 0.9679 - val_loss: 0.3457\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2s/step - accuracy: 0.9677 - loss: 0.3506 - val_accuracy: 0.9679 - val_loss: 0.3114\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9678 - loss: 0.3358 - val_accuracy: 0.9680 - val_loss: 0.2881\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step - accuracy: 0.9678 - loss: 0.3152 - val_accuracy: 0.9681 - val_loss: 0.2970\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 2s/step - accuracy: 0.9679 - loss: 0.3108 - val_accuracy: 0.9680 - val_loss: 0.2680\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 2s/step - accuracy: 0.9679 - loss: 0.2926 - val_accuracy: 0.9680 - val_loss: 0.2580\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 2s/step - accuracy: 0.9679 - loss: 0.2775 - val_accuracy: 0.9681 - val_loss: 0.2444\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9679 - loss: 0.2642 - val_accuracy: 0.9683 - val_loss: 0.2201\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9681 - loss: 0.2259 - val_accuracy: 0.9683 - val_loss: 0.1730\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.9681 - loss: 0.1998 - val_accuracy: 0.9684 - val_loss: 0.1709\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.9680 - loss: 0.1949 - val_accuracy: 0.9684 - val_loss: 0.1632\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.9568 - loss: 0.2217 - val_accuracy: 0.9445 - val_loss: 0.2519\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.9467 - loss: 0.2560 - val_accuracy: 0.9679 - val_loss: 0.1861\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9667 - loss: 0.2084 - val_accuracy: 0.9680 - val_loss: 0.1672\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.9677 - loss: 0.1910 - val_accuracy: 0.9680 - val_loss: 0.1607\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9678 - loss: 0.1843 - val_accuracy: 0.9680 - val_loss: 0.1619\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9679 - loss: 0.1779 - val_accuracy: 0.9684 - val_loss: 0.1541\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - accuracy: 0.9680 - loss: 0.1728 - val_accuracy: 0.9681 - val_loss: 0.1504\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.9680 - loss: 0.1694 - val_accuracy: 0.9682 - val_loss: 0.1519\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step - accuracy: 0.9681 - loss: 0.1645 - val_accuracy: 0.9682 - val_loss: 0.1477\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 2s/step - accuracy: 0.9682 - loss: 0.1596 - val_accuracy: 0.9684 - val_loss: 0.1409\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.9682 - loss: 0.1558 - val_accuracy: 0.9684 - val_loss: 0.1393\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.9682 - loss: 0.1509 - val_accuracy: 0.9684 - val_loss: 0.1368\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(results['with_add_lstm'])\n",
    "plt.xlabel('Epochs');\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model started with 0.9169 accuracy ended. After running 25 epochs with 1000 batch size, the final accuracy was 0.9687. Please experiment the model with different batch sizes, dropout value, optimisers, metrics and layers to get better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.3 environment at E:\\anaconda3\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m41 packages\u001b[0m \u001b[2min 2.36s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m20 packages\u001b[0m \u001b[2min 3.98s\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1.75s\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m20 packages\u001b[0m \u001b[2min 1.00s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mblis\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcatalogue\u001b[0m\u001b[2m==2.0.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcloudpathlib\u001b[0m\u001b[2m==0.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mconfection\u001b[0m\u001b[2m==0.1.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcymem\u001b[0m\u001b[2m==2.0.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangcodes\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlanguage-data\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarisa-trie\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmurmurhash\u001b[0m\u001b[2m==1.0.11\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4 (from file:///C:/b/abs_c1ywpu18ar/croot/numpy_and_numpy_base_1708638681471/work/dist/numpy-1.26.4-cp312-cp312-win_amd64.whl)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpreshed\u001b[0m\u001b[2m==3.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mspacy\u001b[0m\u001b[2m==3.8.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mspacy-legacy\u001b[0m\u001b[2m==3.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mspacy-loggers\u001b[0m\u001b[2m==1.0.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msrsly\u001b[0m\u001b[2m==2.4.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthinc\u001b[0m\u001b[2m==8.3.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwasabi\u001b[0m\u001b[2m==1.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mweasel\u001b[0m\u001b[2m==0.4.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.3 environment at E:\\anaconda3\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 278ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 427ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 418ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 4.14s\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mh5py\u001b[0m\u001b[2m==3.11.0 (from file:///C:/b/abs_c4ha_1xv14/croot/h5py_1715094776210/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh5py\u001b[0m\u001b[2m==3.12.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.1.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install --upgrade numpy h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'E:\\\\anaconda3\\\\Lib\\\\site-packages\\\\numpy\\\\linalg\\\\_umath_linalg.cp312-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.1.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 20.5/60.8 kB 330.3 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 51.2/60.8 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.8/60.8 kB 544.1 kB/s eta 0:00:00\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.12.1-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Downloading numpy-2.1.3-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/12.6 MB 9.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/12.6 MB 9.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.2/12.6 MB 9.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.7/12.6 MB 10.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.2/12.6 MB 10.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.7/12.6 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.2/12.6 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.8/12.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.2/12.6 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.8/12.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.3/12.6 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.3/12.6 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.9/12.6 MB 11.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.4/12.6 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.9/12.6 MB 11.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.4/12.6 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.8/12.6 MB 10.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.3/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.8/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.4/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.4/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading h5py-3.12.1-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.6/3.0 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.0/3.0 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.5/3.0 MB 10.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.0/3.0 MB 11.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.4/3.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/3.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 9.1 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, h5py\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 435.7 kB/s eta 0:00:30\n",
      "     --------------------------------------- 0.1/12.8 MB 871.5 kB/s eta 0:00:15\n",
      "      --------------------------------------- 0.3/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.7/12.8 MB 3.6 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.3/12.8 MB 5.3 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 6.2 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.2/12.8 MB 6.8 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.7/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 4.1/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.6/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 7.9 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.7/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 8.5 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.7/12.8 MB 8.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 8.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.6/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 8.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 8.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.3 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp(\n",
    "    'Jim bought 300 shares of Acme Corp. in 2006. And producing an annotated block of text that \\\n",
    "    highlights the names of entities: [Jim]Person bought 300 shares of \\\n",
    "    [Acme Corp.]Organization in [2006]Time. In this example, a person name consisting \\\n",
    "    of one token, a two-token company name and a temporal expression have been detected \\\n",
    "    and classified.State-of-the-art NER systems for English produce near-human performance. \\\n",
    "    For example, the best system entering MUC-7 scored 93.39% of F-measure while human \\\n",
    "    annotators scored 97.60% and 96.95%.[1][2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jim PERSON\n",
      "300 CARDINAL\n",
      "Acme Corp. ORG\n",
      "2006 DATE\n",
      "300 CARDINAL\n",
      "Acme Corp.]Organization WORK_OF_ART\n",
      "2006]Time DATE\n",
      "one CARDINAL\n",
      "two CARDINAL\n",
      "NER ORG\n",
      "English NORP\n",
      "MUC-7 ORG\n",
      "93.39% PERCENT\n",
      "97.60% PERCENT\n",
      "96.95%.[1][2 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "for w in text.ents:\n",
    "    print(w.text, w.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jim\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " bought \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    300\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " shares of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Acme Corp.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2006\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". And producing an annotated block of text that     highlights the names of entities: [Jim]Person bought \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    300\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " shares of     [\n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Acme Corp.]Organization\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " in [\n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2006]Time\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". In this example, a person name consisting     of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " token, a \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "-token company name and a temporal expression have been detected     and classified.State-of-the-art \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NER\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " systems for \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    English\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " produce near-human performance.     For example, the best system entering \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MUC-7\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " scored \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    93.39%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " of F-measure while human     annotators scored \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    97.60%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    96.95%.[1][2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "]</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(text, style = 'ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numerals that do not fall under another type'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('CARDINAL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
